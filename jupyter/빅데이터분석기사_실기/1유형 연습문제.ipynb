{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615fe77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "target = iris['target']\n",
    "data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "target = pd.DataFrame(data=iris['target'], columns=['species'])\n",
    "df = pd.concat([data, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.  종(species) 별로 petal_width 상위 25%값을 구한 후, 가장 큰값과 가장 작은값 사이의 차이를 출력하시오.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01d75d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('species')['petal width (cm)'].quantile(0.75).max() - df.groupby('species')['petal width (cm)'].quantile(0.75).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adcc8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "result = df.groupby('species').describe()['petal width (cm)',   '75%']\n",
    "print(result.max()-result.min())\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. sepal_length 변수의 전체 값들을 중복 없이, 오른차순으로 정렬했을 때 리스트의 10번째 값을 출력하시오.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f60fe823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "result = df['sepal length (cm)'].unique()\n",
    "result.sort()\n",
    "print(result[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd62272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(df['sepal length (cm)'].unique())\n",
    "print(result.sort_values(0).iloc[9, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. sepal_width 변수 값으로 내림차순 후 위에서부터 100개 행의 표준편차 값을 구한 후 50을 곱한 값을 출력하시오.(단 최종 계산값의 소수점 이하는 버림)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7fa3ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(int(df.sort_values('sepal width (cm)', ascending=False).iloc[0:100,]['sepal width (cm)'].std()*50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3896ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. 종(species) 별로 petal_length 상위 10개 행(큰 값)의 평균값을 구한 후, 평균값의 합을 출력하시오.(소수점 이하 버림)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "67980434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun-jr-902-02\\AppData\\Local\\Temp\\ipykernel_30412\\3832424349.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data = data.groupby('species')['species','petal length (cm)'].head(10)\n"
     ]
    }
   ],
   "source": [
    "data = df.sort_values('petal length (cm)', ascending=False)\n",
    "data = data.groupby('species')['species','petal length (cm)'].head(10)\n",
    "# print(data.groupby('species').mean())\n",
    "print(data.groupby('species').mean().sum()[0])\n",
    "# print(dir(pd.core.groupby.generic.DataFrameGroupBy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. speal_length 변수가 이상치를 가지는 데이터 행의 수를 출력하시오 (단, 이상치 범위는 (평균+2*표준편차) 이상이거나 (평균-2*표준편차) 이하)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3adbe1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "mean = df['sepal length (cm)'].mean()\n",
    "std = df['sepal length (cm)'].std()\n",
    "upper = mean + 2*std\n",
    "lower = mean - 2*std\n",
    "print(len(df[(df['sepal length (cm)'] > upper) | (df['sepal length (cm)'] < lower)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c27db11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function precision_score in module sklearn.metrics._classification:\n",
      "\n",
      "precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "    Compute the precision.\n",
      "    \n",
      "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "    true positives and ``fp`` the number of false positives. The precision is\n",
      "    intuitively the ability of the classifier not to label as positive a sample\n",
      "    that is negative.\n",
      "    \n",
      "    The best value is 1 and the worst value is 0.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like, default=None\n",
      "        The set of labels to include when ``average != 'binary'``, and their\n",
      "        order if ``average is None``. Labels present in the data can be\n",
      "        excluded, for example to calculate a multiclass average ignoring a\n",
      "        majority negative class, while labels not present in the data will\n",
      "        result in 0 components in a macro average. For multilabel targets,\n",
      "        labels are column indices. By default, all labels in ``y_true`` and\n",
      "        ``y_pred`` are used in sorted order.\n",
      "    \n",
      "        .. versionchanged:: 0.17\n",
      "           Parameter `labels` improved for multiclass problem.\n",
      "    \n",
      "    pos_label : str or int, default=1\n",
      "        The class to report if ``average='binary'`` and the data is binary.\n",
      "        If the data are multiclass or multilabel, this will be ignored;\n",
      "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "        scores for that label only.\n",
      "    \n",
      "    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "        This parameter is required for multiclass/multilabel targets.\n",
      "        If ``None``, the scores for each class are returned. Otherwise, this\n",
      "        determines the type of averaging performed on the data:\n",
      "    \n",
      "        ``'binary'``:\n",
      "            Only report results for the class specified by ``pos_label``.\n",
      "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "        ``'micro'``:\n",
      "            Calculate metrics globally by counting the total true positives,\n",
      "            false negatives and false positives.\n",
      "        ``'macro'``:\n",
      "            Calculate metrics for each label, and find their unweighted\n",
      "            mean.  This does not take label imbalance into account.\n",
      "        ``'weighted'``:\n",
      "            Calculate metrics for each label, and find their average weighted\n",
      "            by support (the number of true instances for each label). This\n",
      "            alters 'macro' to account for label imbalance; it can result in an\n",
      "            F-score that is not between precision and recall.\n",
      "        ``'samples'``:\n",
      "            Calculate metrics for each instance, and find their average (only\n",
      "            meaningful for multilabel classification where this differs from\n",
      "            :func:`accuracy_score`).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "        Precision of the positive class in binary classification or weighted\n",
      "        average of the precision of each class for the multiclass task.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "        support for each class.\n",
      "    recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "        number of true positives and ``fn`` the number of false negatives.\n",
      "    PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "        an estimator and some data.\n",
      "    PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "        binary class predictions.\n",
      "    multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "        sample.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    When ``true positive + false positive == 0``, precision returns 0 and\n",
      "    raises ``UndefinedMetricWarning``. This behavior can be\n",
      "    modified with ``zero_division``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import precision_score\n",
      "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "    >>> precision_score(y_true, y_pred, average='macro')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average='micro')\n",
      "    0.33...\n",
      "    >>> precision_score(y_true, y_pred, average='weighted')\n",
      "    0.22...\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.66..., 0.        , 0.        ])\n",
      "    >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.33..., 0.        , 0.        ])\n",
      "    >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "    array([0.33..., 1.        , 1.        ])\n",
      "    >>> # multilabel classification\n",
      "    >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "    >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "    >>> precision_score(y_true, y_pred, average=None)\n",
      "    array([0.5, 1. , 1. ])\n",
      "\n",
      "None\n",
      "['ConfusionMatrixDisplay', 'DetCurveDisplay', 'DistanceMetric', 'PrecisionRecallDisplay', 'RocCurveDisplay', 'SCORERS', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_base', '_classification', '_dist_metrics', '_pairwise_distances_reduction', '_pairwise_fast', '_plot', '_ranking', '_regression', '_scorer', 'accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'auc', 'average_precision_score', 'balanced_accuracy_score', 'brier_score_loss', 'calinski_harabasz_score', 'check_scoring', 'classification_report', 'cluster', 'cohen_kappa_score', 'completeness_score', 'confusion_matrix', 'consensus_score', 'coverage_error', 'd2_absolute_error_score', 'd2_pinball_score', 'd2_tweedie_score', 'davies_bouldin_score', 'dcg_score', 'det_curve', 'euclidean_distances', 'explained_variance_score', 'f1_score', 'fbeta_score', 'fowlkes_mallows_score', 'get_scorer', 'get_scorer_names', 'hamming_loss', 'hinge_loss', 'homogeneity_completeness_v_measure', 'homogeneity_score', 'jaccard_score', 'label_ranking_average_precision_score', 'label_ranking_loss', 'log_loss', 'make_scorer', 'matthews_corrcoef', 'max_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_gamma_deviance', 'mean_pinball_loss', 'mean_poisson_deviance', 'mean_squared_error', 'mean_squared_log_error', 'mean_tweedie_deviance', 'median_absolute_error', 'multilabel_confusion_matrix', 'mutual_info_score', 'nan_euclidean_distances', 'ndcg_score', 'normalized_mutual_info_score', 'pair_confusion_matrix', 'pairwise', 'pairwise_distances', 'pairwise_distances_argmin', 'pairwise_distances_argmin_min', 'pairwise_distances_chunked', 'pairwise_kernels', 'plot_confusion_matrix', 'plot_det_curve', 'plot_precision_recall_curve', 'plot_roc_curve', 'precision_recall_curve', 'precision_recall_fscore_support', 'precision_score', 'r2_score', 'rand_score', 'recall_score', 'roc_auc_score', 'roc_curve', 'silhouette_samples', 'silhouette_score', 'top_k_accuracy_score', 'v_measure_score', 'zero_one_loss']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# print(help(f1_score))\n",
    "print(help(precision_score))\n",
    "print(dir(metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
