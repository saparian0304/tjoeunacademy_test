{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a608a63",
   "metadata": {},
   "source": [
    "## 1. 데이터 분석 연습하기\n",
    "고객 3,500명에 대한 학습용 데이터를 이용하여 성별 예측 모형을 만든 후, 이를 평가용 데이터에 적용하여 얻은 2,482명 고객의 성별 예측값을 다음과 같은 형식의 csv 파일로 생성하시오.\n",
    "(제출한 모델의 성능은 ROC-AUC 평가지표에 따라 채점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a185e66",
   "metadata": {},
   "source": [
    "### 1.1 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0558a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 라이브러리 가져오기\n",
    "import pandas as pd\n",
    "# 주어진 데이터 파일을 모두 읽어서, 데이터 프레임 변수에 저장하기\n",
    "\n",
    "x_train = pd.read_csv('bigData-main/x_train.csv', encoding='ms949')\n",
    "x_test = pd.read_csv('bigData-main/x_test.csv', encoding='ms949')\n",
    "y_train = pd.read_csv('bigData-main/y_train.csv', encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a0b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cust_id      총구매액     최대구매액       환불금액   주구매상품 주구매지점  내점일수   내점당구매건수  \\\n",
      "0        0  68282840  11264000  6860000.0      기타   강남점    19  3.894737   \n",
      "1        1   2136000   2136000   300000.0     스포츠   잠실점     2  1.500000   \n",
      "2        2   3197000   1639000        NaN  남성 캐주얼   관악점     2  2.000000   \n",
      "3        3  16077620   4935000        NaN      기타   광주점    18  2.444444   \n",
      "4        4  29050000  24000000        NaN      보석  본  점     2  1.500000   \n",
      "\n",
      "     주말방문비율  구매주기  \n",
      "0  0.527027    17  \n",
      "1  0.000000     1  \n",
      "2  0.000000     1  \n",
      "3  0.318182    16  \n",
      "4  0.000000    85  \n"
     ]
    }
   ],
   "source": [
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95402f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cust_id       총구매액     최대구매액        환불금액 주구매상품 주구매지점  내점일수    내점당구매건수  \\\n",
      "0     3500   70900400  22000000   4050000.0    골프  부산본점    13   1.461538   \n",
      "1     3501  310533100  38558000  48034700.0   농산물   잠실점    90   2.433333   \n",
      "2     3502  305264140  14825000  30521000.0  가공식품  본  점   101  14.623762   \n",
      "3     3503    7594080   5225000         NaN  주방용품  부산본점     5   2.000000   \n",
      "4     3504    1795790   1411200         NaN   수산품  청량리점     3   2.666667   \n",
      "\n",
      "     주말방문비율  구매주기  \n",
      "0  0.789474    26  \n",
      "1  0.369863     3  \n",
      "2  0.083277     3  \n",
      "3  0.000000    47  \n",
      "4  0.125000     8  \n"
     ]
    }
   ],
   "source": [
    "print(x_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6e5e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cust_id  gender\n",
      "0        0       0\n",
      "1        1       0\n",
      "2        2       1\n",
      "3        3       1\n",
      "4        4       0\n"
     ]
    }
   ],
   "source": [
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b87c8",
   "metadata": {},
   "source": [
    "#### 말 줄임표로 확인이 어려운 데이터를 확인하는 방법\n",
    "(1) T, transpose() 함수 사용해서 열, 행 방향 바꾸기  \n",
    "(2) 전체 칼럼을 출력하는 옵션 사용하기\n",
    " - pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0d38db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cust_id      총구매액     최대구매액       환불금액   주구매상품 주구매지점  내점일수   내점당구매건수  \\\n",
      "0        0  68282840  11264000  6860000.0      기타   강남점    19  3.894737   \n",
      "1        1   2136000   2136000   300000.0     스포츠   잠실점     2  1.500000   \n",
      "2        2   3197000   1639000        NaN  남성 캐주얼   관악점     2  2.000000   \n",
      "3        3  16077620   4935000        NaN      기타   광주점    18  2.444444   \n",
      "4        4  29050000  24000000        NaN      보석  본  점     2  1.500000   \n",
      "\n",
      "     주말방문비율  구매주기  \n",
      "0  0.527027    17  \n",
      "1  0.000000     1  \n",
      "2  0.000000     1  \n",
      "3  0.318182    16  \n",
      "4  0.000000    85  \n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf37fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 10)\n",
      "(2482, 10)\n",
      "(3500, 2)\n"
     ]
    }
   ],
   "source": [
    "# 행 / 열 확인하기\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feec52d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3500 entries, 0 to 3499\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   cust_id  3500 non-null   int64  \n",
      " 1   총구매액     3500 non-null   int64  \n",
      " 2   최대구매액    3500 non-null   int64  \n",
      " 3   환불금액     1205 non-null   float64\n",
      " 4   주구매상품    3500 non-null   object \n",
      " 5   주구매지점    3500 non-null   object \n",
      " 6   내점일수     3500 non-null   int64  \n",
      " 7   내점당구매건수  3500 non-null   float64\n",
      " 8   주말방문비율   3500 non-null   float64\n",
      " 9   구매주기     3500 non-null   int64  \n",
      "dtypes: float64(3), int64(5), object(2)\n",
      "memory usage: 273.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 요약 정보 확인하기\n",
    "print(x_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab341dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count          mean           std         min           25%  \\\n",
      "cust_id  3500.0  1.749500e+03  1.010507e+03         0.0  8.747500e+02   \n",
      "총구매액     3500.0  9.191925e+07  1.635065e+08 -52421520.0  4.747050e+06   \n",
      "최대구매액    3500.0  1.966424e+07  3.199235e+07  -2992000.0  2.875000e+06   \n",
      "환불금액     1205.0  2.407822e+07  4.746453e+07      5600.0  2.259000e+06   \n",
      "내점일수     3500.0  1.925371e+01  2.717494e+01         1.0  2.000000e+00   \n",
      "내점당구매건수  3500.0  2.834963e+00  1.912368e+00         1.0  1.666667e+00   \n",
      "주말방문비율   3500.0  3.072463e-01  2.897516e-01         0.0  2.729090e-02   \n",
      "구매주기     3500.0  2.095829e+01  2.474868e+01         0.0  4.000000e+00   \n",
      "\n",
      "                  50%           75%           max  \n",
      "cust_id  1.749500e+03  2.624250e+03  3.499000e+03  \n",
      "총구매액     2.822270e+07  1.065079e+08  2.323180e+09  \n",
      "최대구매액    9.837000e+06  2.296250e+07  7.066290e+08  \n",
      "환불금액     7.392000e+06  2.412000e+07  5.637530e+08  \n",
      "내점일수     8.000000e+00  2.500000e+01  2.850000e+02  \n",
      "내점당구매건수  2.333333e+00  3.375000e+00  2.208333e+01  \n",
      "주말방문비율   2.564103e-01  4.489796e-01  1.000000e+00  \n",
      "구매주기     1.300000e+01  2.800000e+01  1.660000e+02  \n"
     ]
    }
   ],
   "source": [
    "# 기초 통계량 확인하기\n",
    "print(x_train.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594caccc",
   "metadata": {},
   "source": [
    "### 1.2 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e528fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터의 cust_id 값은 x_test_cust_id 변수에 저장하기\n",
    "# 분석과정에는 불필요하나 추후 제출에 필요하다\n",
    "x_test_cust_id = x_test['cust_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca97ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       총구매액     최대구매액       환불금액   주구매상품 주구매지점  내점일수   내점당구매건수    주말방문비율  구매주기\n",
      "0  68282840  11264000  6860000.0      기타   강남점    19  3.894737  0.527027    17\n",
      "1   2136000   2136000   300000.0     스포츠   잠실점     2  1.500000  0.000000     1\n",
      "2   3197000   1639000        NaN  남성 캐주얼   관악점     2  2.000000  0.000000     1\n",
      "3  16077620   4935000        NaN      기타   광주점    18  2.444444  0.318182    16\n",
      "4  29050000  24000000        NaN      보석  본  점     2  1.500000  0.000000    85\n",
      "   gender\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       0\n"
     ]
    }
   ],
   "source": [
    "# cust_id 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns=['cust_id'])\n",
    "y_train = y_train.drop(columns=['cust_id'])\n",
    "x_test = x_test.drop(columns=['cust_id'])\n",
    "\n",
    "# 칼럼이 삭제된 상위 5개 행을 확인하기\n",
    "print(x_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f87130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총구매액          0\n",
      "최대구매액         0\n",
      "환불금액       2295\n",
      "주구매상품         0\n",
      "주구매지점         0\n",
      "내점일수          0\n",
      "내점당구매건수       0\n",
      "주말방문비율        0\n",
      "구매주기          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치 처리하기\n",
    "print(x_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47db62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# '환불금액' 칼럼의 결측치는 0으로 대치하기\n",
    "x_train['환불금액'] = x_train['환불금액'].fillna(0)\n",
    "x_test['환불금액'] = x_test['환불금액'].fillna(0)\n",
    "# 결측치가 조치되었는지, '환불금액' 칼럼의 결측치 개수를 확인하기\n",
    "print(x_train['환불금액'].isnull().sum())\n",
    "print(x_test['환불금액'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5182e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기타' '스포츠' '남성 캐주얼' '보석' '디자이너' '시티웨어' '명품' '농산물' '화장품' '골프' '구두' '가공식품'\n",
      " '수산품' '아동' '차/커피' '캐주얼' '섬유잡화' '육류' '축산가공' '젓갈/반찬' '액세서리' '피혁잡화' '일용잡화'\n",
      " '주방가전' '주방용품' '건강식품' '가구' '주류' '모피/피혁' '남성 트랜디' '셔츠' '남성정장' '생활잡화'\n",
      " '트래디셔널' '란제리/내의' '커리어' '침구/수예' '대형가전' '통신/컴퓨터' '식기' '소형가전' '악기']\n",
      "42\n",
      "['강남점' '잠실점' '관악점' '광주점' '본  점' '일산점' '대전점' '부산본점' '분당점' '영등포점' '미아점'\n",
      " '청량리점' '안양점' '부평점' '동래점' '포항점' '노원점' '창원점' '센텀시티점' '인천점' '대구점' '전주점'\n",
      " '울산점' '상인점']\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# 범주형 변수를 인코딩하기\n",
    "\n",
    "# '주구매상품' 칼럼에서 중복을 제외한 값들을 확인하기\n",
    "print(x_train['주구매상품'].unique())\n",
    "# '주구매상품' 칼럼에서 중복을 제외한 값들의 개수 세기\n",
    "print(x_train['주구매상품'].unique().size)\n",
    "# '주구매지점' 칼럼에서 중복을 제외한 값들을 확인하기\n",
    "print(x_train['주구매지점'].unique())\n",
    "# '주구매지점' 칼럼에서 중복을 제외한 값들의 개수 세기\n",
    "print(x_train['주구매지점'].unique().size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f72f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 라벨 인코딩 수행을 위한 encoder 함수 만들기\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f4c1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5\n",
      "1    21\n",
      "2     6\n",
      "3     5\n",
      "4    15\n",
      "5    11\n",
      "6    22\n",
      "7    13\n",
      "8     5\n",
      "9     9\n",
      "Name: 주구매상품, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# '주구매상품'에 대해 라벨 인코딩을 수행한 후, '주구매상품'칼럼으로 다시 저장하기\n",
    "x_train['주구매상품'] = encoder.fit_transform(x_train['주구매상품'])\n",
    "# 라벨인코딩 결과를 확인하기 위해, 상위 10개 행을 확인하기\n",
    "print(x_train['주구매상품'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff5dc0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가공식품' '가구' '건강식품' '골프' '구두' '기타' '남성 캐주얼' '남성 트랜디' '남성정장' '농산물' '대형가전'\n",
      " '디자이너' '란제리/내의' '명품' '모피/피혁' '보석' '생활잡화' '섬유잡화' '셔츠' '소형가전' '수산품' '스포츠'\n",
      " '시티웨어' '식기' '아동' '악기' '액세서리' '육류' '일용잡화' '젓갈/반찬' '주류' '주방가전' '주방용품'\n",
      " '차/커피' '축산가공' '침구/수예' '캐주얼' '커리어' '통신/컴퓨터' '트래디셔널' '피혁잡화' '화장품']\n",
      "['가공식품' '가구' '건강식품' '골프' '구두' '기타' '남성 캐주얼' '남성 트랜디' '남성정장' '농산물' '대형가전'\n",
      " '디자이너' '란제리/내의' '명품' '모피/피혁' '보석' '생활잡화' '섬유잡화' '셔츠' '수산품' '스포츠' '시티웨어'\n",
      " '식기' '아동' '악기' '액세서리' '육류' '일용잡화' '젓갈/반찬' '주류' '주방가전' '주방용품' '차/커피'\n",
      " '축산가공' '침구/수예' '캐주얼' '커리어' '통신/컴퓨터' '트래디셔널' '피혁잡화' '화장품']\n"
     ]
    }
   ],
   "source": [
    "# '주구매상품' 칼럼에 대한 라벨 인코딩의 변환 순서 확인하기\n",
    "print(encoder.classes_)\n",
    "\n",
    "# 테스트 데이터로 라벨 인코딩 수행하기\n",
    "x_test['주구매상품'] = encoder.fit_transform(x_test['주구매상품'])\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45474227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train컬럼이 더 많은 경우\n",
    "if len(x_train.columns) >= len(x_test.columns) :\n",
    "    x_train = x_train[x_test.columns]\n",
    "else : # x_test컬럼이 더 많은 경우\n",
    "    x_test = x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08575d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     546\n",
      "1      10\n",
      "2      47\n",
      "3      82\n",
      "4      54\n",
      "5     595\n",
      "6      55\n",
      "7       2\n",
      "8      22\n",
      "9     339\n",
      "10      8\n",
      "11    193\n",
      "12      8\n",
      "13    100\n",
      "14     57\n",
      "15      3\n",
      "16     15\n",
      "17     98\n",
      "18     30\n",
      "19      2\n",
      "20    153\n",
      "21     69\n",
      "22    213\n",
      "23      7\n",
      "24     40\n",
      "25      2\n",
      "26      5\n",
      "27     57\n",
      "28     64\n",
      "29     29\n",
      "30     14\n",
      "31     26\n",
      "32     32\n",
      "33     44\n",
      "34     35\n",
      "35      4\n",
      "36    101\n",
      "37      9\n",
      "38      3\n",
      "39     23\n",
      "40     40\n",
      "41    264\n",
      "Name: 주구매상품, dtype: int64\n",
      "0     395\n",
      "1       7\n",
      "2      36\n",
      "3      47\n",
      "4      44\n",
      "5     465\n",
      "6      46\n",
      "7       4\n",
      "8      25\n",
      "9     235\n",
      "10      6\n",
      "11    123\n",
      "12     10\n",
      "13     60\n",
      "14     47\n",
      "15      1\n",
      "16      8\n",
      "17     72\n",
      "18     25\n",
      "19     86\n",
      "20     43\n",
      "21    168\n",
      "22      8\n",
      "23     24\n",
      "24      1\n",
      "25      7\n",
      "26     38\n",
      "27     30\n",
      "28     18\n",
      "29     10\n",
      "30     10\n",
      "31     21\n",
      "32     55\n",
      "33     28\n",
      "34      3\n",
      "35     65\n",
      "36      6\n",
      "37      3\n",
      "38      9\n",
      "39     16\n",
      "40    177\n",
      "Name: 주구매상품, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "######### 물어볼 내용 ###########\n",
    "# fit_transform 은 처음에만하고\n",
    "# test 데이터에는 transform 만 해야하나?\n",
    "print(x_train['주구매상품'].value_counts().sort_index())\n",
    "print(x_test['주구매상품'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf73e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1    19\n",
      "2     1\n",
      "3     2\n",
      "4     8\n",
      "5    18\n",
      "6     0\n",
      "7     8\n",
      "8     8\n",
      "9     5\n",
      "Name: 주구매지점, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# '주구매지점'에 대해 라벨 인코딩을 수행한 후, '주구매지점' 칼럼으로 다시 저장하기\n",
    "x_train['주구매지점'] = encoder.fit_transform(x_train['주구매지점'])\n",
    "\n",
    "# 라벨인코딩 결과를 확인하기 위해, 상위 10개 행을 확인하기\n",
    "print(x_train['주구매지점'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339bd571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['강남점' '관악점' '광주점' '노원점' '대구점' '대전점' '동래점' '미아점' '본  점' '부산본점' '부평점' '분당점'\n",
      " '상인점' '센텀시티점' '안양점' '영등포점' '울산점' '인천점' '일산점' '잠실점' '전주점' '창원점' '청량리점'\n",
      " '포항점']\n",
      "['강남점' '관악점' '광주점' '노원점' '대구점' '대전점' '동래점' '미아점' '본  점' '부산본점' '부평점' '분당점'\n",
      " '상인점' '센텀시티점' '안양점' '영등포점' '울산점' '인천점' '일산점' '잠실점' '전주점' '창원점' '청량리점'\n",
      " '포항점']\n"
     ]
    }
   ],
   "source": [
    "# '주구매지점' 칼럼에 대한 라벨 인코딩의 변환 순서 확인하기\n",
    "print(encoder.classes_)\n",
    "# 테스트 데이터의 '주구매지점' 칼럼도 라벨 인코딩 수행하기\n",
    "x_test['주구매지점'] = encoder.fit_transform(x_test['주구매지점'])\n",
    "print(encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e079ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '환불금액' 칼럼이 0보다 큰지에 대한 조건을 condition 변수에 저장하기\n",
    "condition = x_train['환불금액'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dbd27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition 조건에 맞으면 '환불금액_new' 칼럼을 1로 설정하기\n",
    "x_train.loc[condition,'환불금액_new'] = 1\n",
    "# condition 조건이 맞지 않으면, '환불금액_new' 칼럼을 0으로 설정하기\n",
    "x_train.loc[~condition,'환불금액_new'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f550e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           환불금액  환불금액_new\n",
      "0     6860000.0       1.0\n",
      "1      300000.0       1.0\n",
      "2           0.0       0.0\n",
      "3           0.0       0.0\n",
      "4           0.0       0.0\n",
      "...         ...       ...\n",
      "3495        0.0       0.0\n",
      "3496  6049600.0       1.0\n",
      "3497        0.0       0.0\n",
      "3498        0.0       0.0\n",
      "3499  5973000.0       1.0\n",
      "\n",
      "[3500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# '환불금액', '환불금액_new' 칼럼을 비교하기\n",
    "print(x_train[['환불금액', '환불금액_new']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2322dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '환불금액'칼럼의 삭제 내용은 바로 데이터 세트에 반영하기\n",
    "x_train = x_train.drop(columns=['환불금액'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86bd877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 세트에서도 '환불금액_new'라는 파생변수 만들기(~는 not의 의미)\n",
    "x_test.loc[condition, '환불금액_new'] = 1\n",
    "x_test.loc[~condition, '환불금액_new'] = 0\n",
    "\n",
    "# '환불금액' 칼럼의 삭제 내용은 바로 데이터 세트에 반영하기\n",
    "x_test = x_test.drop(columns=['환불금액'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e4cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count          mean           std         min           25%  \\\n",
      "총구매액      3500.0  9.191925e+07  1.635065e+08 -52421520.0  4.747050e+06   \n",
      "최대구매액     3500.0  1.966424e+07  3.199235e+07  -2992000.0  2.875000e+06   \n",
      "주구매상품     3500.0  1.461200e+01  1.301995e+01         0.0  5.000000e+00   \n",
      "주구매지점     3500.0  1.073429e+01  5.636480e+00         0.0  8.000000e+00   \n",
      "내점일수      3500.0  1.925371e+01  2.717494e+01         1.0  2.000000e+00   \n",
      "내점당구매건수   3500.0  2.834963e+00  1.912368e+00         1.0  1.666667e+00   \n",
      "주말방문비율    3500.0  3.072463e-01  2.897516e-01         0.0  2.729090e-02   \n",
      "구매주기      3500.0  2.095829e+01  2.474868e+01         0.0  4.000000e+00   \n",
      "환불금액_new  3500.0  3.442857e-01  4.752027e-01         0.0  0.000000e+00   \n",
      "\n",
      "                   50%           75%           max  \n",
      "총구매액      2.822270e+07  1.065079e+08  2.323180e+09  \n",
      "최대구매액     9.837000e+06  2.296250e+07  7.066290e+08  \n",
      "주구매상품     9.000000e+00  2.200000e+01  4.100000e+01  \n",
      "주구매지점     9.000000e+00  1.500000e+01  2.300000e+01  \n",
      "내점일수      8.000000e+00  2.500000e+01  2.850000e+02  \n",
      "내점당구매건수   2.333333e+00  3.375000e+00  2.208333e+01  \n",
      "주말방문비율    2.564103e-01  4.489796e-01  1.000000e+00  \n",
      "구매주기      1.300000e+01  2.800000e+01  1.660000e+02  \n",
      "환불금액_new  0.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# 크기변환 전, x_train 세트의 기초 통계량 확인하기\n",
    "print(x_train.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb735a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지의 preprocessing 모듈에서 StandardScaler 함수를 가져오기\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 표준화 크기 변환을 수행하기 위한 scaler 객체를 만들기\n",
    "scaler = StandardScaler()\n",
    "# scaler 객체로 표준화 크기변환을 수행하고, x_train의 칼럼명을 사용하기\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd5e65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터는 transform() 함수만으로 값을 변환\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c86592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count          mean       std       min       25%       50%  \\\n",
      "총구매액      3500.0 -4.409171e-17  1.000143 -0.882909 -0.533218 -0.389621   \n",
      "최대구매액     3500.0 -4.838986e-17  1.000143 -0.708278 -0.524864 -0.307219   \n",
      "주구매상품     3500.0 -6.236281e-17  1.000143 -1.122438 -0.738357 -0.431093   \n",
      "주구매지점     3500.0 -8.333017e-17  1.000143 -1.904703 -0.485175 -0.307733   \n",
      "내점일수      3500.0  2.518303e-16  1.000143 -0.671807 -0.635003 -0.414180   \n",
      "내점당구매건수   3500.0 -2.288170e-16  1.000143 -0.959661 -0.611003 -0.262346   \n",
      "주말방문비율    3500.0  9.192647e-17  1.000143 -1.060530 -0.966329 -0.175472   \n",
      "구매주기      3500.0 -1.220294e-16  1.000143 -0.846966 -0.685318 -0.321610   \n",
      "환불금액_new  3500.0  3.445498e-16  1.000143 -0.724606 -0.724606 -0.724606   \n",
      "\n",
      "               75%        max  \n",
      "총구매액      0.089237  13.648260  \n",
      "최대구매액     0.103110  21.475852  \n",
      "주구매상품     0.567518   2.027026  \n",
      "주구매지점     0.756913   2.176441  \n",
      "내점일수      0.211486   9.780490  \n",
      "내점당구매건수   0.282432  10.066639  \n",
      "주말방문비율    0.489224   2.391196  \n",
      "구매주기      0.284570   5.861421  \n",
      "환불금액_new  1.380060   1.380060  \n"
     ]
    }
   ],
   "source": [
    "# 크기 변환 후, x_train 세트의 기초 통계량 확인하기\n",
    "print(x_train.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dd37927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              총구매액     최대구매액  환불금액_new\n",
      "총구매액      1.000000  0.700080  0.403357\n",
      "최대구매액     0.700080  1.000000  0.330687\n",
      "환불금액_new  0.403357  0.330687  1.000000\n"
     ]
    }
   ],
   "source": [
    "# '총구매액', '최대구매액', '환불금액_new' 칼럼 간의 상관관계 구하기\n",
    "print(x_train[['총구매액', '최대구매액', '환불금액_new']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba15c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train 세트에서 '최대구매액' 칼럼을 삭제한 후, x_train에 저장하기\n",
    "x_train = x_train.drop(columns=['최대구매액'])\n",
    "# x_test 세트에서 '최대구매액' 칼럼을 삭제한 후, x_test에 저장하기\n",
    "x_test = x_test.drop(columns=['최대구매액'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26170059",
   "metadata": {},
   "source": [
    "### 1.3 학습하고 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "043bb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  1\n",
      "1  0\n",
      "2  1\n",
      "<class 'numpy.ndarray'>\n",
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# sklearn 패키지의 tree 모듈에서 DecisionTreeClassifier 함수를 가져오기\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# 의사결정나무 방식으로 수행되는 model 객체를 만들기\n",
    "model = DecisionTreeClassifier()\n",
    "# x_train, y_train 세트로 model을 학습시키기\n",
    "model.fit(x_train, y_train)\n",
    "# 학습된 model을 활용하여 테스트 데이터의 종속변수를 예측하기\n",
    "y_test_predicted = model.predict(x_test)\n",
    "# 예측한 결과를 출력해보기\n",
    "print(pd.DataFrame(y_test_predicted).head(3))\n",
    "print(type(y_test_predicted))\n",
    "print(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5594bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  1\n",
      "1  0\n",
      "2  1\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 튜닝\n",
    "# 괄호 안에 하이퍼 파라미터를 입력하여 모델 객체를 생성하기\n",
    "model = DecisionTreeClassifier(max_depth=10, criterion='entropy', random_state=10)\n",
    "# x_train, y_train 세트로 model을 학습시키기\n",
    "model.fit(x_train, y_train)\n",
    "# 학습된 model을 활용하여 테스트 데이터의 종속변수를 예측하기\n",
    "y_test_predicted = model.predict(x_test)\n",
    "# 예측한 결과를 출력해보기\n",
    "print(pd.DataFrame(y_test_predicted).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebe631",
   "metadata": {},
   "source": [
    "### GridSearchCV() 함수로 하이퍼파라미터 튜닝하기\n",
    "- 하이퍼파라미터의 최적 조합을 찾을 때 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2423ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝을 위한 GridSearchCV 함수 가져오기\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 병렬처리를 위한 Pipeline 함수 가져오기\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd19034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터들과 튜닝할 값들을 작성하기\n",
    "parameters = {'model__max_depth' : [3,4,5,6],\n",
    "             'model__criterion' : ['gini', 'entropy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15a25a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스케일링과 의사결정나무 분류작업을 병렬처리하도록 모델 만들기\n",
    "pipeline_model = Pipeline([('scaler', StandardScaler()), ('model', DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d63e84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model = GridSearchCV(pipeline_model, param_grid=parameters, cv=3) # cv : 교차검증 수행횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfb48272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GridSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class GridSearchCV(BaseSearchCV)\n",
      " |  GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      " |  \n",
      " |  Exhaustive search over specified parameter values for an estimator.\n",
      " |  \n",
      " |  Important members are fit, predict.\n",
      " |  \n",
      " |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      " |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      " |  implemented in the estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated grid-search over a parameter grid.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <grid_search>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_grid : dict or list of dictionaries\n",
      " |      Dictionary with parameters names (`str`) as keys and lists of\n",
      " |      parameter settings to try as values, or a list of such\n",
      " |      dictionaries, in which case the grids spanned by each dictionary\n",
      " |      in the list are explored. This enables searching over any sequence\n",
      " |      of parameter settings.\n",
      " |  \n",
      " |  scoring : str, callable, list, tuple or dict, default=None\n",
      " |      Strategy to evaluate the performance of the cross-validated model on\n",
      " |      the test set.\n",
      " |  \n",
      " |      If `scoring` represents a single score, one can use:\n",
      " |  \n",
      " |      - a single string (see :ref:`scoring_parameter`);\n",
      " |      - a callable (see :ref:`scoring`) that returns a single value.\n",
      " |  \n",
      " |      If `scoring` represents multiple scores, one can use:\n",
      " |  \n",
      " |      - a list or tuple of unique strings;\n",
      " |      - a callable returning a dictionary where the keys are the metric\n",
      " |        names and the values are the metric scores;\n",
      " |      - a dictionary with metric names as keys and callables a values.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionchanged:: v0.20\n",
      " |         `n_jobs` default changed from 1 to None\n",
      " |  \n",
      " |  refit : bool, str, or callable, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      Where there are considerations other than maximum score in\n",
      " |      choosing a best estimator, ``refit`` can be set to a function which\n",
      " |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      " |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      " |      according to the returned ``best_index_`` while the ``best_score_``\n",
      " |      attribute will not be available.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``GridSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |      See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n",
      " |      to see how to design a custom selection strategy using a callable\n",
      " |      via `refit`.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Support for callable added.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      " |      with `shuffle=False` so the splits will be the same across calls.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  verbose : int\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |      - >1 : the computation time for each fold and parameter candidate is\n",
      " |        displayed;\n",
      " |      - >2 : the score is also displayed;\n",
      " |      - >3 : the fold and candidate parameter indexes are also displayed\n",
      " |        together with the starting time of the computation.\n",
      " |  \n",
      " |  pre_dispatch : int, or str, default='2*n_jobs'\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A str, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  error_score : 'raise' or numeric, default=np.nan\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : bool, default=False\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |          Default value was changed from ``True`` to ``False``\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      " |      +============+===========+============+=================+===+=========+\n",
      " |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      " |      +------------+-----------+------------+-----------------+---+---------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      " |                                       mask = [False False False False]...)\n",
      " |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      " |                                      mask = [ True  True False False]...),\n",
      " |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      " |                                       mask = [False False  True  True]...),\n",
      " |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      " |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      " |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      " |          'rank_test_score'    : [2, 4, 3, 1],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      " |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |      This attribute is not available if ``refit`` is a function.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      " |      specified.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  multimetric_ : bool\n",
      " |      Whether or not the scorers compute several metrics.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels. This is present only if ``refit`` is specified and\n",
      " |      the underlying estimator is a classifier.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`. Only defined if\n",
      " |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      " |      parameter for more details) and that `best_estimator_` exposes\n",
      " |      `n_features_in_` when fit.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Only defined if\n",
      " |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      " |      parameter for more details) and that `best_estimator_` exposes\n",
      " |      `feature_names_in_` when fit.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
      " |  train_test_split : Utility function to split the data into a development\n",
      " |      set usable for fitting a GridSearchCV instance and an evaluation set\n",
      " |      for its final evaluation.\n",
      " |  sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      " |      loss function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the left out\n",
      " |  data, unless an explicit score is passed in which case it is used instead.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import svm, datasets\n",
      " |  >>> from sklearn.model_selection import GridSearchCV\n",
      " |  >>> iris = datasets.load_iris()\n",
      " |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      " |  >>> svc = svm.SVC()\n",
      " |  >>> clf = GridSearchCV(svc, parameters)\n",
      " |  >>> clf.fit(iris.data, iris.target)\n",
      " |  GridSearchCV(estimator=SVC(),\n",
      " |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      " |  >>> sorted(clf.cv_results_.keys())\n",
      " |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      " |   'param_C', 'param_kernel', 'params',...\n",
      " |   'rank_test_score', 'split0_test_score',...\n",
      " |   'split2_test_score', ...\n",
      " |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GridSearchCV\n",
      " |      BaseSearchCV\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Result of the decision function for `X` based on the estimator with\n",
      " |          the best found parameters.\n",
      " |  \n",
      " |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like of shape (n_samples,), default=None\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      " |      \n",
      " |      **fit_params : dict of str -> object\n",
      " |          Parameters passed to the `fit` method of the estimator.\n",
      " |      \n",
      " |          If a fit parameter is an array-like whose length is equal to\n",
      " |          `num_samples` then it will be split across CV groups along with `X`\n",
      " |          and `y`. For example, the :term:`sample_weight` parameter is split\n",
      " |          because `len(sample_weights) = len(X)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Instance of fitted estimator.\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Result of the `inverse_transform` function for `Xt` based on the\n",
      " |          estimator with the best found parameters.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          The predicted labels or values for `X` based on the estimator with\n",
      " |          the best found parameters.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Predicted class log-probabilities for `X` based on the estimator\n",
      " |          with the best found parameters. The order of the classes\n",
      " |          corresponds to that in the fitted attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Predicted class probabilities for `X` based on the estimator with\n",
      " |          the best found parameters. The order of the classes corresponds\n",
      " |          to that in the fitted attribute :term:`classes_`.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Return the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The score defined by ``scoring`` if provided, and the\n",
      " |          ``best_estimator_.score`` method otherwise.\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Call score_samples on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``score_samples``.\n",
      " |      \n",
      " |      .. versionadded:: 0.24\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements\n",
      " |          of the underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : ndarray of shape (n_samples,)\n",
      " |          The ``best_estimator_.score_samples`` method.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          `X` transformed in the new space based on the estimator with\n",
      " |          the best found parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |      Class labels.\n",
      " |      \n",
      " |      Only available when `refit=True` and the estimator is a classifier.\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |      \n",
      " |      Only available when `refit=True`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(GridSearchCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73653b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3,\n",
      "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                                       ('model', DecisionTreeClassifier())]),\n",
      "             param_grid={'model__criterion': ['gini', 'entropy'],\n",
      "                         'model__max_depth': [3, 4, 5, 6]})\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.fit(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa19134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c91441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "...  ..\n",
      "2477  1\n",
      "2478  0\n",
      "2479  0\n",
      "2480  0\n",
      "2481  1\n",
      "\n",
      "[2482 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = best_model.predict(x_test)\n",
    "print(pd.DataFrame(y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecdae07",
   "metadata": {},
   "source": [
    "### GridSearchCV 내용 끝  \n",
    "### 결과 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "191da593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0  0.200000  0.800000\n",
      "1  1.000000  0.000000\n",
      "2  0.000000  1.000000\n",
      "3  0.548837  0.451163\n",
      "4  0.548837  0.451163\n"
     ]
    }
   ],
   "source": [
    "# model을 통해 x_test에 맞는 종속변수 확률을 구하기\n",
    "y_test_proba = model.predict_proba(x_test)\n",
    "# 종속변수의 0, 1에 대한 각 확률을 확인하기\n",
    "print(pd.DataFrame(y_test_proba).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4f8e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.800000\n",
      "1       0.000000\n",
      "2       1.000000\n",
      "3       0.451163\n",
      "4       0.451163\n",
      "          ...   \n",
      "2477    0.462312\n",
      "2478    0.451163\n",
      "2479    0.338028\n",
      "2480    0.000000\n",
      "2481    0.512821\n",
      "Name: 1, Length: 2482, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 성별이 남성인 확률값을 데이터 프레임 형태로 출력하기 \n",
    "print(pd.DataFrame(y_test_proba)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e122964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 결과를 result 변수에 저장하기 \n",
    "result = pd.DataFrame(y_test_proba)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fee1d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6896432468240978\n"
     ]
    }
   ],
   "source": [
    "# x_train에 대한 종속 변수를 예측하기\n",
    "y_train_predicted = model.predict(x_train)\n",
    "# roc 평가지표를 계산하기 위한 함수를 가져오기\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# 학습데이터에 대한 y_train, y_train_predicted의 ROC 결과 확인하기\n",
    "print(roc_auc_score(y_train, y_train_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c00370",
   "metadata": {},
   "source": [
    "### 테스트데이터로 모델 평가하기\n",
    "- 주어진 학습데이터를 분리하여 검증용 데이터로 평가하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67545b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 8)\n",
      "(700, 8)\n",
      "(2800, 1)\n",
      "(700, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리하기 위한 train_test_split 함수를 가져오기\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 학습용과 검증용을 8:2로 분리하기\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "# 분리된 데이터의 행/열 구조를 확인하기\n",
    "print(X_TRAIN.shape)\n",
    "print(X_TEST.shape)\n",
    "print(Y_TRAIN.shape)\n",
    "print(Y_TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1641f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사결정나무 모델을 생성하기\n",
    "model = DecisionTreeClassifier(max_depth=10, criterion='entropy', random_state=10)\n",
    "# 분리된 X_TRAIN, Y_TRAIN으로 모델을 학습시키기\n",
    "model.fit(X_TRAIN, Y_TRAIN)\n",
    "# 학습한 모델을 통해 X_TEST 세트에 대응되는 Y_TEST 세트를 예측하기\n",
    "Y_TEST_PREDICTED = model.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2f1505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5771332492643968\n"
     ]
    }
   ],
   "source": [
    "# sklearn 패키지의 metrics 모듈에서 roc_auc_score 함수를 가져오기\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# roc 평가지표 값을 확인하기\n",
    "print(roc_auc_score(Y_TEST, Y_TEST_PREDICTED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055be7d",
   "metadata": {},
   "source": [
    "### 1.4 결과 제출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "507633de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3501</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3502</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3503</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3504</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>5977</td>\n",
       "      <td>0.462312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>5978</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.338028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>5980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>5981</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id         1\n",
       "0        3500  0.800000\n",
       "1        3501  0.000000\n",
       "2        3502  1.000000\n",
       "3        3503  0.451163\n",
       "4        3504  0.451163\n",
       "...       ...       ...\n",
       "2477     5977  0.462312\n",
       "2478     5978  0.451163\n",
       "2479     5979  0.338028\n",
       "2480     5980  0.000000\n",
       "2481     5981  0.512821\n",
       "\n",
       "[2482 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test_cust_id 변수와 result 변수를 세로 방향으로 붙이기\n",
    "pd.concat([x_test_cust_id, result], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e563f51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3500</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3501</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3502</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3503</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3504</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>5977</td>\n",
       "      <td>0.462312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>5978</td>\n",
       "      <td>0.451163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>5979</td>\n",
       "      <td>0.338028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>5980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>5981</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cust_id    gender\n",
       "0        3500  0.800000\n",
       "1        3501  0.000000\n",
       "2        3502  1.000000\n",
       "3        3503  0.451163\n",
       "4        3504  0.451163\n",
       "...       ...       ...\n",
       "2477     5977  0.462312\n",
       "2478     5978  0.451163\n",
       "2479     5979  0.338028\n",
       "2480     5980  0.000000\n",
       "2481     5981  0.512821\n",
       "\n",
       "[2482 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '1' 칼럼명을 'gender' 칼럼명으로 변환하여 다시 결과를 확인하기\n",
    "pd.concat([x_test_cust_id, result], axis=1).rename(columns={1:'gender'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1de5af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 출력 결과를 final 변수에 저장하기\n",
    "final = pd.concat([x_test_cust_id, result], axis=1).rename(columns={1:'gender'})\n",
    "# final 변수를 data 디렉터리 하위에 12345.csv 이름으로 저장하기\n",
    "final.to_csv('data/12345.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10886c5",
   "metadata": {},
   "source": [
    "### 제출된 파일이 정상적으로 저장되었는지 다시 확인하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffb8d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cust_id    gender\n",
      "0        3500  0.800000\n",
      "1        3501  0.000000\n",
      "2        3502  1.000000\n",
      "3        3503  0.451163\n",
      "4        3504  0.451163\n",
      "...       ...       ...\n",
      "2477     5977  0.462312\n",
      "2478     5978  0.451163\n",
      "2479     5979  0.338028\n",
      "2480     5980  0.000000\n",
      "2481     5981  0.512821\n",
      "\n",
      "[2482 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# data 디렉터리 하위의 12345.csv 파일을 final 변수에 저장하기\n",
    "final = pd.read_csv('data/12345.csv')\n",
    "# final 변수의 데이터 확인하기\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfd512cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cust_id    gender\n",
      "0        3500  0.000000\n",
      "1        3501  0.194215\n",
      "2        3502  0.000000\n",
      "3        3503  0.490909\n",
      "4        3504  1.000000\n",
      "...       ...       ...\n",
      "2477     5977  0.393443\n",
      "2478     5978  0.533632\n",
      "2479     5979  0.000000\n",
      "2480     5980  0.285714\n",
      "2481     5981  0.000000\n",
      "\n",
      "[2482 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 최종 제출 코드\n",
    "# 라이브러리와 파일 읽기\n",
    "import pandas as pd\n",
    "x_train = pd.read_csv('bigData-main/x_train.csv', encoding='ms949')\n",
    "x_test = pd.read_csv('bigData-main/x_test.csv', encoding='ms949')\n",
    "y_train = pd.read_csv('bigData-main/y_train.csv', encoding='ms949')\n",
    "\n",
    "# 테스트 데이터의 cust_id 저장하기\n",
    "x_test_cust_id = x_test['cust_id']\n",
    "\n",
    "# cust_id 컬럼 삭제하기\n",
    "x_train = x_train.drop(columns=['cust_id'])\n",
    "x_test = x_test.drop(columns=['cust_id'])\n",
    "y_train = y_train.drop(columns=['cust_id'])\n",
    "\n",
    "# 결측치 처리하기 \n",
    "x_train['환불금액'] = x_train['환불금액'].fillna(0)\n",
    "x_test['환불금액'] = x_test['환불금액'].fillna(0)\n",
    "\n",
    "# 라벨인코딩 수행하기\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "x_train['주구매상품'] = encoder.fit_transform(x_train['주구매상품'])\n",
    "x_test['주구매상품'] = encoder.transform(x_test['주구매상품'])\n",
    "x_train['주구매지점'] = encoder.fit_transform(x_train['주구매지점'])\n",
    "x_test['주구매지점'] = encoder.transform(x_test['주구매지점'])\n",
    "\n",
    "# 파생변수 만들기\n",
    "condition = x_train['환불금액'] > 0\n",
    "x_train.loc[condition, '환불금액_new'] = 1\n",
    "x_train.loc[~condition, '환불금액_new'] = 0\n",
    "x_train = x_train.drop(columns=['환불금액'])\n",
    "x_test.loc[condition, '환불금액_new'] = 1\n",
    "x_test.loc[~condition, '환불금액_new'] = 0\n",
    "x_test = x_test.drop(columns=['환불금액'])\n",
    "\n",
    "# 데이터 스케일링 : 표준화 크기로 변환하기\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "\n",
    "# 불필요한 칼럼 삭제하기\n",
    "x_train = x_train.drop(columns=['최대구매액'])\n",
    "x_test = x_test.drop(columns=['최대구매액'])\n",
    "\n",
    "# 학습용/검증용 데이터로 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "\n",
    "# 모델 학습 및 평가하기\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=10, criterion='entropy', random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)\n",
    "y_test_proba = model.predict_proba(x_test)\n",
    "\n",
    "# 결과 제출하기\n",
    "result = pd.DataFrame(y_test_proba)[1]\n",
    "final = pd.concat([x_test_cust_id, result], axis=1).rename(columns={1:'gender'})\n",
    "print(final)\n",
    "final.to_csv('data/12345.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2fbb0",
   "metadata": {},
   "source": [
    "## 2. 분류모델 수행하기\n",
    "고객 891명에 대한 학습용 데이터(titanic_x_train.csv, titanic_y_train.csv)를 이용하여 생존여부를 예측하는 모형을 만듭니다. 이후 예측 모형을 평가용 데이터(titanic_x_test.csv)에 적용하여 418명 승객의 생존 여부 예측값을 다음과 같은 형식의 csv 파일로 생성하시오.\n",
    "(제출한 모델의 성능은 ROC-AUC 평가지표에 따라 채점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d134bb4",
   "metadata": {},
   "source": [
    "### 2.1 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06f37985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 라이브러리 가져오기\n",
    "import pandas as pd \n",
    "\n",
    "# 주어진 데이터 파일을 모두 읽어서, 각 데이터 프레임 변수에 저장하기\n",
    "x_train = pd.read_csv('bigData-main/titanic_x_train.csv', encoding='cp949')\n",
    "x_test = pd.read_csv('bigData-main/titanic_x_test.csv', encoding='cp949')\n",
    "y_train = pd.read_csv('bigData-main/titanic_y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3948acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   0\n",
      "PassengerId                        1\n",
      "티켓등급                               3\n",
      "승객이름         Braund, Mr. Owen Harris\n",
      "성별                              male\n",
      "나이                              22.0\n",
      "형제자매배우자수                           1\n",
      "부모자식수                              0\n",
      "티켓번호                       A/5 21171\n",
      "운임요금                            7.25\n",
      "객실번호                             NaN\n",
      "선착장                                S\n",
      "                            0\n",
      "PassengerId               892\n",
      "티켓등급                        3\n",
      "승객이름         Kelly, Mr. James\n",
      "성별                       male\n",
      "나이                       34.5\n",
      "형제자매배우자수                    0\n",
      "부모자식수                       0\n",
      "티켓번호                   330911\n",
      "운임요금                   7.8292\n",
      "객실번호                      NaN\n",
      "선착장                         Q\n"
     ]
    }
   ],
   "source": [
    "# x_train, x_test의 상위 1개 행을 확인하기\n",
    "print(x_train.head(1).T)\n",
    "print(x_test.head(1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd1a82ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived\n",
      "0            1         0\n",
      "1            2         1\n",
      "2            3         1\n",
      "3            4         1\n",
      "4            5         0\n"
     ]
    }
   ],
   "source": [
    "# y_train의 상위 5개 행을 가져오기\n",
    "print(y_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19c2c81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11)\n",
      "(418, 11)\n",
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터 세트의 행과 열의 개수를 확인하기\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "179263ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   티켓등급         891 non-null    int64  \n",
      " 2   승객이름         891 non-null    object \n",
      " 3   성별           891 non-null    object \n",
      " 4   나이           714 non-null    float64\n",
      " 5   형제자매배우자수     891 non-null    int64  \n",
      " 6   부모자식수        891 non-null    int64  \n",
      " 7   티켓번호         891 non-null    object \n",
      " 8   운임요금         891 non-null    float64\n",
      " 9   객실번호         204 non-null    object \n",
      " 10  선착장          889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# x_train 세트의 요약정보 확인하기\n",
    "print(x_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3848ea9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# '성별' 칼럼의 중복 제거한 값과 개수 확인하기\n",
    "print(x_train['성별'].unique().size)\n",
    "print(x_train['성별'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dfe1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# '선착장' 칼럼의 중복 제거한 값과 개수 확인하기\n",
    "print(x_train['선착장'].unique().size)\n",
    "print(x_train['선착장'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47fb3cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "681\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# 나머지 3개의 칼럼의 중복 제거한 값의 개수 확인하기\n",
    "print(x_train['승객이름'].unique().size)\n",
    "print(x_train['티켓번호'].unique().size)\n",
    "print(x_train['객실번호'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "652ebb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             count        mean         std   min       25%       50%    75%  \\\n",
      "PassengerId  891.0  446.000000  257.353842  1.00  223.5000  446.0000  668.5   \n",
      "티켓등급         891.0    2.308642    0.836071  1.00    2.0000    3.0000    3.0   \n",
      "나이           714.0   29.699118   14.526497  0.42   20.1250   28.0000   38.0   \n",
      "형제자매배우자수     891.0    0.523008    1.102743  0.00    0.0000    0.0000    1.0   \n",
      "부모자식수        891.0    0.381594    0.806057  0.00    0.0000    0.0000    0.0   \n",
      "운임요금         891.0   32.204208   49.693429  0.00    7.9104   14.4542   31.0   \n",
      "\n",
      "                  max  \n",
      "PassengerId  891.0000  \n",
      "티켓등급           3.0000  \n",
      "나이            80.0000  \n",
      "형제자매배우자수       8.0000  \n",
      "부모자식수          6.0000  \n",
      "운임요금         512.3292  \n"
     ]
    }
   ],
   "source": [
    "# x_train의 기초 통계량을 확인하고, 가독성을 위해 행/열 바꿔서 출력하기\n",
    "print(x_train.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c38c0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train을 세로 방향으로 통합한 후, data 변수에 저장하기\n",
    "data = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9877c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성별\n",
      "female    0.742038\n",
      "male      0.188908\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# '성별' 칼럼에 따라 Survived의 평균값을 구하기\n",
    "print(data.groupby(['성별'])['Survived'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a744288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "티켓등급\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# '티켓등급' 칼럼에 따라 Survived의 평균값을 구하기\n",
    "print(data.groupby(['티켓등급'])['Survived'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b279e5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선착장\n",
      "C    0.553571\n",
      "Q    0.389610\n",
      "S    0.336957\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# '선착장' 칼럼에 따라 Survived의 평균값을 구하기\n",
    "print(data.groupby(['선착장'])['Survived'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b605d2c",
   "metadata": {},
   "source": [
    "### 2.2 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7f4b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터의 PassengerID 값은 x_test_passenger_id 변수에 저장하기\n",
    "x_test_passenger_id = x_test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8b3fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived\n",
      "0            1         0\n",
      "1            2         1\n",
      "2            3         1\n",
      "3            4         1\n",
      "4            5         0\n"
     ]
    }
   ],
   "source": [
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9bc5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   티켓등급                                               승객이름      성별    나이  \\\n",
      "0     3                            Braund, Mr. Owen Harris    male  22.0   \n",
      "1     1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
      "2     3                             Heikkinen, Miss. Laina  female  26.0   \n",
      "3     1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
      "4     3                           Allen, Mr. William Henry    male  35.0   \n",
      "\n",
      "   형제자매배우자수  부모자식수              티켓번호     운임요금  객실번호 선착장  \n",
      "0         1      0         A/5 21171   7.2500   NaN   S  \n",
      "1         1      0          PC 17599  71.2833   C85   C  \n",
      "2         0      0  STON/O2. 3101282   7.9250   NaN   S  \n",
      "3         1      0            113803  53.1000  C123   S  \n",
      "4         0      0            373450   8.0500   NaN   S  \n",
      "   Survived\n",
      "0         0\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         0\n"
     ]
    }
   ],
   "source": [
    "# PassengerId 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns = ['PassengerId'])\n",
    "x_test = x_test.drop(columns = ['PassengerId'])\n",
    "y_train = y_train.drop(columns = ['PassengerId'])\n",
    "\n",
    "# 칼럼이 삭제된 상위 5개 행을 확인하기\n",
    "print(x_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20c2b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 테스트 데이터에서 '티켓번호'칼럼 삭제하기\n",
    "x_train = x_train.drop(columns = ['티켓번호'])\n",
    "x_test = x_test.drop(columns = ['티켓번호'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2fb0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터와 테스트데이터에서 '승객이름'칼럼 삭제하기\n",
    "x_train = x_train.drop(columns=['승객이름'])\n",
    "x_test = x_test.drop(columns=['승객이름'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05d48ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "687\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# '나이', '객실번호', '선착장' 칼럼의 중복 제외한 값 개수 세기\n",
    "print(x_train['나이'].isnull().sum())\n",
    "print(x_train['객실번호'].isnull().sum())\n",
    "print(x_train['선착장'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "548c1fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                나이  Survived\n",
      "나이        1.000000 -0.077221\n",
      "Survived -0.077221  1.000000\n"
     ]
    }
   ],
   "source": [
    "# '나이'와 'Survived' 칼럼 간의 상관관계를 확인하기\n",
    "print(data[['나이', 'Survived']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26f0f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train에서 '나이' 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns = ['나이'])\n",
    "# x_test에서 '나이' 칼럼을 삭제하기\n",
    "x_test = x_test.drop(columns = ['나이'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fdc582a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "# x_train의 '객실번호'값에서 중복을 제외한 값의 개수 세기\n",
    "print(x_train['객실번호'].unique().size)\n",
    "# x_train에서 '객실번호' 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns = ['객실번호'])\n",
    "# x_test 세트에서 '객실번호' 칼럼을 삭제하기\n",
    "x_test = x_test.drop(columns = ['객실번호'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab8bedc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선착장\n",
      "C    168\n",
      "Q     77\n",
      "S    644\n",
      "Name: 선착장, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# '선착장' 칼럼별로 값의 개수를 세기\n",
    "print(x_train.groupby(['선착장'])['선착장'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49b88e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: 선착장, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(x_train['선착장'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97705b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# x_train의 '선착장' 칼럼 결측치는 'S' 값으로 적용하기\n",
    "x_train['선착장'] = x_train['선착장'].fillna('S')\n",
    "# x_train의 '선착장' 칼럼에 결측치가 있는지 다시 확인하기\n",
    "print(x_train['선착장'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4780025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# x_test의 '선착장' 칼럼에 결측치가 있는지 다시 확인하기\n",
    "print(x_test['선착장'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87916eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터의 '성별' 칼럼을 인코딩하기\n",
    "x_train['성별'] = x_train['성별'].replace('male', 0).replace('female', 1)\n",
    "# 테스트 데이터의 '성별' 칼럼을 인코딩하기\n",
    "x_test['성별'] = x_test['성별'].replace('male', 0).replace('female', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7b9f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   티켓등급  성별  형제자매배우자수  부모자식수     운임요금 선착장  선착장Q  선착장S\n",
      "0     3   0         1      0   7.2500   S     0     1\n",
      "1     1   1         1      0  71.2833   C     0     0\n",
      "2     3   1         0      0   7.9250   S     0     1\n",
      "3     1   1         1      0  53.1000   S     0     1\n",
      "4     3   0         0      0   8.0500   S     0     1\n"
     ]
    }
   ],
   "source": [
    "# x_train의 '선착장'칼럼에 대해 원핫인코딩을 수행한 후, '선착장_dummy'에 저장하기\n",
    "선착장_dummy = pd.get_dummies(x_train['선착장'], drop_first=True).rename(columns={'Q':'선착장Q', 'S':'선착장S'})\n",
    "# 기존 x_train의 우측에 '선착장_dummy' 변수를 덧붙여, x_train에 다시 저장하기\n",
    "x_train = pd.concat([x_train, 선착장_dummy], axis=1)\n",
    "# x_train의 상위 5개 행 확인하기\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "32a788c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터에서 '선착장' 칼럼 삭제하기\n",
    "x_train = x_train.drop(columns=['선착장'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "534aca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test 세트의 선착장 칼럼에 대해 원핫인코딩을 수행하고, 기존 칼럼은 삭제하기\n",
    "선착장_dummy = pd.get_dummies(x_test['선착장'], drop_first=True).rename(columns={'Q':'선착장Q', 'S':'선착장S'})\n",
    "# 기존 x_test의 우측에 '선착장_dummy'변수를 덧붙여, x_test에 다시 저장하기\n",
    "x_test = pd.concat([x_test, 선착장_dummy], axis=1)\n",
    "# 테스트 데이터에서 '선착장' 칼럼 삭제하기\n",
    "x_test = x_test.drop(columns=['선착장'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58280db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   형제자매배우자수  부모자식수  가족수\n",
      "0         1      0    1\n",
      "1         1      0    1\n",
      "2         0      0    0\n",
      "3         1      0    1\n",
      "4         0      0    0\n",
      "5         0      0    0\n",
      "6         0      0    0\n",
      "7         3      1    4\n",
      "8         0      2    2\n",
      "9         1      0    1\n"
     ]
    }
   ],
   "source": [
    "# '형제자매배우자수'와 '부모자식수' 칼럼을 더하여 '가족수'라는 칼럼을 만들기\n",
    "x_train['가족수'] = x_train['형제자매배우자수'] + x_train['부모자식수']\n",
    "# 결과 확인을 위해, 상위 10개 행을 확인하기\n",
    "print(x_train[['형제자매배우자수', '부모자식수', '가족수']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1fd15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터에서 '형제자매배우자수', '부모자식수' 칼럼 삭제하기\n",
    "x_train = x_train.drop(columns = ['형제자매배우자수', '부모자식수'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d9244642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '형제자매배우자수'와 '부모자식수'칼럼을 더한 '가족수'라는 칼럼을 만들기\n",
    "x_test['가족수'] = x_test['형제자매배우자수'] + x_test['부모자식수']\n",
    "# 테스트 데이터에서 '형제자매배우자수', '부모자식수' 칼럼 삭제하기\n",
    "x_test = x_test.drop(columns=['형제자매배우자수', '부모자식수'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f3e27",
   "metadata": {},
   "source": [
    "### 2.3 학습하고 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38b2fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 6)\n",
      "(179, 6)\n",
      "(712, 1)\n",
      "(179, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리하기 위한 train_test_split 함수를 가져오기\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 학습용과 검증용을 8:2로 분리하여, 각 4개의 변수에 저장하기\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "# 분리된 데이터의 행/열 구조를 확인하기\n",
    "print(X_TRAIN.shape)\n",
    "print(X_TEST.shape)\n",
    "print(Y_TRAIN.shape)\n",
    "print(Y_TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94c56804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost 라이브러리에서 제공하는 XGBClassifier 함수를 가져오기\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6252ef8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;error&#x27;, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;error&#x27;, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='error', gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB 분류 분석을 수행할 첫번째 모델을 만들고, 공부시키기\n",
    "model = XGBClassifier(eval_metric = 'error', random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a6b5e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;error&#x27;, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=&#x27;error&#x27;, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='error', gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=10,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB 분류 분석을 수행할 두 번째 모델을 만들기\n",
    "model = XGBClassifier(n_estimators = 100, max_depth=5, eval_metric='error', random_state=10)\n",
    "# 만들어진 모델을 공부시키기\n",
    "model.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c3df009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived\n",
      "0         0\n",
      "1         1\n",
      "2         0\n",
      "3         0\n",
      "4         1\n",
      "5         0\n",
      "6         0\n",
      "7         1\n",
      "8         1\n",
      "9         0\n"
     ]
    }
   ],
   "source": [
    "# 학습이 완료된 모델을 통해 y_test 값을 예측하기: 최종 결과 제출용\n",
    "y_test_predicted = pd.DataFrame(model.predict(x_test)).rename(columns={0:'Survived'})\n",
    "\n",
    "# y_test_predicted 값 확인하기\n",
    "print(pd.DataFrame(y_test_predicted).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2619258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 통해 Y_TEST 값을 예측하기 : 평가지표 계산용\n",
    "Y_TEST_PREDICTED = pd.DataFrame(model.predict(X_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "276c8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지의 metrics 모듈에서 roc_auc_score 함수를 가져오기\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4efed270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7983181692859112\n"
     ]
    }
   ],
   "source": [
    "# 1차 학습 모델의 ROC 평가지표 값을 확인하기\n",
    "print(roc_auc_score(Y_TEST, Y_TEST_PREDICTED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ae50d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7983181692859112\n"
     ]
    }
   ],
   "source": [
    "# 2차 학습 모델의 ROC 평가지표 값을 확인하기\n",
    "print(roc_auc_score(Y_TEST, Y_TEST_PREDICTED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea2a05",
   "metadata": {},
   "source": [
    "### 2.4 결과 제출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "06b30405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         1\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         1\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# x_test_passenger_id 변수와 y_test_predicted 변수를 세로 방향으로 붙이기\n",
    "print(pd.concat([x_test_passenger_id, y_test_predicted], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0373040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 출력 결과를 final 변수에 저장하기\n",
    "final = pd.concat([x_test_passenger_id, y_test_predicted], axis=1)\n",
    "# final 변수를 data 디렉터리 하위에 12345.csv 이름으로 저장하기\n",
    "final.to_csv('data/12345.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f2689b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 제출 코드\n",
    "# 데이터 가져오기\n",
    "import pandas as pd\n",
    "x_train = pd.read_csv('bigData-main/titanic_x_train.csv', encoding='ms949')\n",
    "x_test = pd.read_csv('bigData-main/titanic_x_test.csv', encoding='ms949')\n",
    "y_train = pd.read_csv('bigData-main/titanic_y_train.csv')\n",
    "# 전처리하기\n",
    "x_test_passenger_id = x_test['PassengerId']\n",
    "x_train = x_train.drop(columns = ['PassengerId'])\n",
    "x_test = x_test.drop(columns = ['PassengerId'])\n",
    "y_train = y_train.drop(columns = ['PassengerId'])\n",
    "x_train = x_train.drop(columns=['티켓번호'])\n",
    "x_test = x_test.drop(columns = ['티켓번호'])\n",
    "x_train = x_train.drop(columns=['승객이름'])\n",
    "x_test = x_test.drop(columns=['승객이름'])\n",
    "x_train = x_train.drop(columns=['나이'])\n",
    "x_test = x_test.drop(columns=['나이'])\n",
    "x_train = x_train.drop(columns=['객실번호'])\n",
    "x_test = x_test.drop(columns=['객실번호'])\n",
    "# 결측치 처리하기\n",
    "x_train['선착장'] = x_train['선착장'].fillna('S')\n",
    "x_test['선착장'] = x_test['선착장'].fillna('S')\n",
    "\n",
    "# 인코딩 수행하기\n",
    "x_train['성별'] = x_train['성별'].replace('male', 0).replace('female', 1)\n",
    "x_test['성별'] = x_test['성별'].replace('male', 0).replace('female', 1)\n",
    "선착장_dummy = pd.get_dummies(x_train['선착장'], drop_first=True).rename(columns={'Q':'선착장Q', 'S':'선착장S'})\n",
    "x_train = pd.concat([x_train, 선착장_dummy], axis=1)\n",
    "x_train = x_train.drop(columns=['선착장'])\n",
    "선착장_dummy = pd.get_dummies(x_test['선착장'], drop_first=True).rename(columns={'Q':'선착장Q', 'S':'선착장S'})\n",
    "x_test = pd.concat([x_test, 선착장_dummy], axis=1)\n",
    "x_test = x_test.drop(columns=['선착장'])\n",
    "\n",
    "# 파생변수 만들기\n",
    "x_train['가족수'] = x_train['형제자매배우자수'] + x_train['부모자식수']\n",
    "x_train = x_train.drop(columns = ['형제자매배우자수', '부모자식수'])\n",
    "x_test['가족수'] = x_test['형제자매배우자수'] + x_test['부모자식수']\n",
    "x_test = x_test.drop(columns = ['형제자매배우자수', '부모자식수'])\n",
    "\n",
    "# 데이터 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "\n",
    "# 모델 학습하기\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(n_estimators=100, max_depth=5, eval_metric='error', random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)\n",
    "\n",
    "# 최종 결과를 파일로 저장하기\n",
    "y_test_predicted = pd.DataFrame(model.predict(x_test)).rename(columns={0:'Survived'})\n",
    "final = pd.concat([x_test_passenger_id, y_test_predicted], axis=1)\n",
    "final.to_csv('data/12345.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f82de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         1\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         1\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv('data/12345.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edc317",
   "metadata": {},
   "source": [
    "## 3. 예측모델 수행하기\n",
    "고객 10,866건에 대학 학습용 데이터(bike_x_train.csv, bike_y_train.csv)를 이용하여 자전거 대여량 예측 모형을 만든다. 생성한 예측 모형으로 평가용 데이터(bike_x_test.csv)에 해당하는 6,493건의 자전거 대여량 예측값을 다음과 같은 형식의 csv 파일로 생성하시오. \n",
    "(제출한 모델의 성능은 R^2 score 평가지표에 따라 채점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fe6aa",
   "metadata": {},
   "source": [
    "### 3.1 데이터 탐색하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a6e21e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 라이브러리 가져오기\n",
    "import pandas as pd\n",
    "\n",
    "# 주어진 데이터 파일을 모두 읽어서, 각 데이터 프레임 변수에 저장하기\n",
    "x_train = pd.read_csv('bigData-main/bike_x_train.csv', encoding='ms949')\n",
    "x_test = pd.read_csv('bigData-main/bike_x_test.csv', encoding='ms949')\n",
    "y_train = pd.read_csv('bigData-main/bike_y_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "08438ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          datetime  계절  공휴일  근무일  날씨    온도    체감온도  습도   풍속\n",
      "0  2011-01-01 0:00   1    0    0   1  9.84  14.395  81  0.0\n",
      "1  2011-01-01 1:00   1    0    0   1  9.02  13.635  80  0.0\n",
      "2  2011-01-01 2:00   1    0    0   1  9.02  13.635  80  0.0\n"
     ]
    }
   ],
   "source": [
    "# 독립변수인 x_train, x_test의 상위 3개 행을 확인하기\n",
    "print(x_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5c091c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          datetime  계절  공휴일  근무일  날씨     온도    체감온도  습도       풍속\n",
      "0  2011-01-20 0:00   1    0    1   1  10.66  11.365  56  26.0027\n",
      "1  2011-01-20 1:00   1    0    1   1  10.66  13.635  56   0.0000\n",
      "2  2011-01-20 2:00   1    0    1   1  10.66  13.635  56   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(x_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6f4551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          datetime  count\n",
      "0  2011-01-01 0:00     16\n",
      "1  2011-01-01 1:00     40\n",
      "2  2011-01-01 2:00     32\n"
     ]
    }
   ],
   "source": [
    "# 종속변수인 y_train의 상위 3개 행을 확인하기\n",
    "print(y_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c19d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 9)\n",
      "(6493, 9)\n",
      "(10886, 2)\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터 세트의 행과 열 개수를 확인하기\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04ece199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   datetime  10886 non-null  object \n",
      " 1   계절        10886 non-null  int64  \n",
      " 2   공휴일       10886 non-null  int64  \n",
      " 3   근무일       10886 non-null  int64  \n",
      " 4   날씨        10886 non-null  int64  \n",
      " 5   온도        10886 non-null  float64\n",
      " 6   체감온도      10886 non-null  float64\n",
      " 7   습도        10886 non-null  int64  \n",
      " 8   풍속        10886 non-null  float64\n",
      "dtypes: float64(3), int64(5), object(1)\n",
      "memory usage: 765.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# x_train 세트의 요약정보 확인하기\n",
    "print(x_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bdac86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# 범주형 변수들의 값을 확인하기\n",
    "print(x_train['계절'].unique())\n",
    "print(x_train['공휴일'].unique())\n",
    "print(x_train['근무일'].unique())\n",
    "print(x_train['날씨'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5abe3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min      25%     50%      75%       max\n",
      "계절    10886.0   2.506614   1.116174  1.00   2.0000   3.000   4.0000    4.0000\n",
      "공휴일   10886.0   0.028569   0.166599  0.00   0.0000   0.000   0.0000    1.0000\n",
      "근무일   10886.0   0.680875   0.466159  0.00   0.0000   1.000   1.0000    1.0000\n",
      "날씨    10886.0   1.418427   0.633839  1.00   1.0000   1.000   2.0000    4.0000\n",
      "온도    10886.0  20.230860   7.791590  0.82  13.9400  20.500  26.2400   41.0000\n",
      "체감온도  10886.0  23.655084   8.474601  0.76  16.6650  24.240  31.0600   45.4550\n",
      "습도    10886.0  61.886460  19.245033  0.00  47.0000  62.000  77.0000  100.0000\n",
      "풍속    10886.0  12.799395   8.164537  0.00   7.0015  12.998  16.9979   56.9969\n"
     ]
    }
   ],
   "source": [
    "# x_train의 기초 통계량을 확인하고, 가독성을 위해 행/열 바꿔서 출력하기\n",
    "print(x_train.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb76f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train을 세로방향으로 통합한 후, data 변수에 저장하기\n",
    "data = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76a038bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계절\n",
      "1    312498\n",
      "2    588282\n",
      "3    640662\n",
      "4    544034\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# '계절'칼럼에 따른 count(자전거 대여량)합계를 구하기\n",
    "# 1(봄), 2(여름), 3(가을), 4(겨울)\n",
    "print(data.groupby(['계절'])['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35e05f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공휴일\n",
      "0    2027668\n",
      "1      57808\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# '공휴일' 칼럼에 따른 count(자전거 대여량) 합계를 구하기\n",
    "# 0(공휴일 아님), 1(공휴일)\n",
    "print(data.groupby(['공휴일'])['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6418262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근무일\n",
      "0     654872\n",
      "1    1430604\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# '근무일' 칼럼에 따른 count(자전거 대여량) 합계를 구하기\n",
    "# 0 (근무일 아님), 1 (근무일)\n",
    "print(data.groupby(['근무일'])['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9597cc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날씨\n",
      "1    1476063\n",
      "2     507160\n",
      "3     102089\n",
      "4        164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# '날씨' 칼럼에 따른 count(자전거 대여량) 합계를 구하기\n",
    "# 1 - 아주 깨끗한 날씨\n",
    "# 2 - 안개와 구름이 있는 날씨\n",
    "# 3 - 조금의 눈과 비 또는 조금의 천둥이 치는 날씨\n",
    "# 4 - 아주 많은 비 또는 우박이 내리는 날씨\n",
    "print(data.groupby(['날씨'])['count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94321425",
   "metadata": {},
   "source": [
    "### 3.2 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c36e2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime 칼럼의 데이터 타입을 날짜 타입(datetime)으로 변환하기\n",
    "x_train['datetime'] = pd.to_datetime(x_train['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "859f4035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2011 2012]\n"
     ]
    }
   ],
   "source": [
    "# x_train의 'datetime' 칼럼에서 연도 데이터를 추출하여 'year' 칼럼에 저장하기\n",
    "x_train['year'] = x_train['datetime'].dt.year\n",
    "# x_train의 'year' 칼럼에서 중복 제거한 값을 확인하기\n",
    "print(x_train['year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ff89434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "# x_train의 'datetime' 칼럼에서 월 데이터를 추출하여 'month'칼럼에 저장하기\n",
    "x_train['month'] = x_train['datetime'].dt.month\n",
    "# x_train의 'month' 칼럼에서 중복 제거한 값을 확인하기\n",
    "print(x_train['month'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fb1a5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "# x_train의 'datetime' 칼럼에서 시간 데이터를 추출하여 'day' 칼럼에 저장하기\n",
    "x_train['day'] = x_train['datetime'].dt.day\n",
    "# x_train의 'day' 칼럼에서 중복 제거한 값을 확인하기\n",
    "print(x_train['day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "666214d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n"
     ]
    }
   ],
   "source": [
    "# x_train의 'datetime' 칼럼에서 시간 데이터를 추출하여 'hour' 칼럼에 저장하기\n",
    "x_train['hour'] = x_train['datetime'].dt.hour\n",
    "# x_train의 'hour' 칼럼에서 중복 제거한 값을 확인하기\n",
    "print(x_train['hour'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7017503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# x_train의 'datetime'칼럼에서 요일 데이터를 추출하여 'dayofweek' 칼럼에 저장하기\n",
    "x_train['dayofweek'] = x_train['datetime'].dt.dayofweek\n",
    "# x_train의 'dayofweek'(요일) 칼럼에서 중복 제거한 값을 확인하기\n",
    "print(x_train['dayofweek'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ca4c6e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  계절  공휴일  근무일  날씨    온도    체감온도  습도   풍속  year  month  \\\n",
      "0 2011-01-01 00:00:00   1    0    0   1  9.84  14.395  81  0.0  2011      1   \n",
      "1 2011-01-01 01:00:00   1    0    0   1  9.02  13.635  80  0.0  2011      1   \n",
      "2 2011-01-01 02:00:00   1    0    0   1  9.02  13.635  80  0.0  2011      1   \n",
      "3 2011-01-01 03:00:00   1    0    0   1  9.84  14.395  75  0.0  2011      1   \n",
      "4 2011-01-01 04:00:00   1    0    0   1  9.84  14.395  75  0.0  2011      1   \n",
      "\n",
      "   day  hour  dayofweek         datetime  count  \n",
      "0    1     0          5  2011-01-01 0:00     16  \n",
      "1    1     1          5  2011-01-01 1:00     40  \n",
      "2    1     2          5  2011-01-01 2:00     32  \n",
      "3    1     3          5  2011-01-01 3:00     13  \n",
      "4    1     4          5  2011-01-01 4:00      1  \n"
     ]
    }
   ],
   "source": [
    "# 파생 변수가 포함된 독립변수와 종속변수를 통합한 data2 만들기\n",
    "data2 = pd.concat([x_train, y_train], axis=1)\n",
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34813fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2011     781979\n",
      "2012    1303497\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'year' 칼럼에 따른 'count' 합계를 구하기\n",
    "print(data2.groupby(['year'])['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16ec5c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "1      79884\n",
      "2      99113\n",
      "3     133501\n",
      "4     167402\n",
      "5     200147\n",
      "6     220733\n",
      "7     214617\n",
      "8     213516\n",
      "9     212529\n",
      "10    207434\n",
      "11    176440\n",
      "12    160160\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'month'칼럼에 따른 'count' 합계를 구하기\n",
    "print(data2.groupby(['month'])['count'].sum())\n",
    "# x_train에서 'month'칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e6c1d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day\n",
      "1     103692\n",
      "2     105381\n",
      "3     111561\n",
      "4     112335\n",
      "5     109115\n",
      "6     108600\n",
      "7     105486\n",
      "8     102770\n",
      "9     108041\n",
      "10    111645\n",
      "11    111146\n",
      "12    109257\n",
      "13    111448\n",
      "14    112406\n",
      "15    115677\n",
      "16    109837\n",
      "17    118255\n",
      "18    108437\n",
      "19    110387\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'day' 칼럼에 따른 'count' 합계를 구하기\n",
    "print(data2.groupby(['day'])['count'].sum())\n",
    "# x_train에서 'day' 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "54000161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour\n",
      "0      25088\n",
      "1      15372\n",
      "2      10259\n",
      "3       5091\n",
      "4       2832\n",
      "5       8935\n",
      "6      34698\n",
      "7      96968\n",
      "8     165060\n",
      "9     100910\n",
      "10     79667\n",
      "11     95857\n",
      "12    116968\n",
      "13    117551\n",
      "14    111010\n",
      "15    115960\n",
      "16    144266\n",
      "17    213757\n",
      "18    196472\n",
      "19    143767\n",
      "20    104204\n",
      "21     79057\n",
      "22     60911\n",
      "23     40816\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'hour' 칼럼에 따른 'count' 합계를 구하기\n",
    "print(data2.groupby(['hour'])['count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3c5462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayofweek\n",
      "0    295296\n",
      "1    291985\n",
      "2    292226\n",
      "3    306401\n",
      "4    302504\n",
      "5    311518\n",
      "6    285546\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['dayofweek'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [167]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data2\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayofweek\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# x_train에서 'dayofweek' 칼럼을 삭제하기\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdayofweek\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\kdigital_a\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\kdigital_a\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\kdigital_a\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mE:\\kdigital_a\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\kdigital_a\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['dayofweek'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# 'dayofweek' 칼럼에 따른 'count' 합계를 구하기\n",
    "print(data2.groupby(['dayofweek'])['count'].sum())\n",
    "# x_train에서 'dayofweek' 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns=['dayofweek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d74bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 세트도 동일하게 적용하기\n",
    "x_test['datetime'] = pd.to_datetime(x_test['datetime'])\n",
    "x_test['year'] = x_test['datetime'].dt.year\n",
    "x_test['hour'] = x_test['datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac38dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test의 datetime 칼럼 값은 x_test_datetime 변수에 저장하기\n",
    "x_test_datetime = x_test['datetime']\n",
    "# x_train, x_test에서 datetime 칼럼을 삭제하기\n",
    "x_train = x_train.drop(columns=['datetime'])\n",
    "x_test = x_test.drop(columns=['datetime'])\n",
    "y_train = y_train.drop(columns=['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579c2bd",
   "metadata": {},
   "source": [
    "### 3.3 학습하고 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cba08b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8708, 10)\n",
      "(2178, 10)\n",
      "(8708, 1)\n",
      "(2178, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 분리하기 위한 train_test_split 함수를 가져오기\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 학습용과 검증용을 8:2로 분리하여, 각 4개의 변수에 저장하기\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "# 분리된 데이터의 행/열 구조를 확인하기\n",
    "print(X_TRAIN.shape)\n",
    "print(X_TEST.shape)\n",
    "print(Y_TRAIN.shape)\n",
    "print(Y_TEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9bf5ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost 라이브러리에서 XGBRegressor 함수 가져오기 \n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "13b5d0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법 1\n",
    "# XGB 회귀 분석으로 수행할 첫번째 모델을 만들고, 공부시키기\n",
    "model = XGBRegressor(n_estimators=100, max_depth=3, random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "05b269e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      계절  공휴일  근무일  날씨     온도    체감온도  습도       풍속  year  hour\n",
      "7038   2    0    1   3  15.58  19.695  46  15.0013  2012    15\n",
      "4114   4    0    1   2  16.40  20.455  71  15.0013  2011    11\n",
      "5720   1    0    1   1   9.84   9.850  38  32.9975  2012    12\n",
      "   계절  공휴일  근무일  날씨     온도    체감온도  습도       풍속  year  hour\n",
      "0   1    0    1   1  10.66  11.365  56  26.0027  2011     0\n",
      "1   1    0    1   1  10.66  13.635  56   0.0000  2011     1\n",
      "2   1    0    1   1  10.66  13.635  56   0.0000  2011     2\n"
     ]
    }
   ],
   "source": [
    "print(X_TRAIN.head(3))\n",
    "print(x_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "827f5888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count\n",
      "2     -8.262762\n",
      "3     -9.796791\n",
      "4     -9.796791\n",
      "23   -17.563259\n",
      "26    -2.508549\n",
      "...         ...\n",
      "6449  -7.557519\n",
      "6450 -17.969536\n",
      "6451 -13.125678\n",
      "6471  -2.048431\n",
      "6472  -3.502649\n",
      "\n",
      "[384 rows x 1 columns]\n",
      "        count\n",
      "0    2.139193\n",
      "1    0.979986\n",
      "2    0.000000\n",
      "3    0.000000\n",
      "4    0.000000\n",
      "5    7.486256\n",
      "6   79.849319\n",
      "7  141.700363\n",
      "8  319.266785\n",
      "9   91.251663\n"
     ]
    }
   ],
   "source": [
    "# 학습이 완료된 모델을 통해 y_test 값을 예측하기 : 최종 결과 제출용\n",
    "y_test_predicted = pd.DataFrame(model.predict(x_test)).rename(columns={0:'count'})\n",
    "# y_test_predicted의 'count' 칼럼 값이 음수인 데이터를 추출하여, 0으로 바꾸기\n",
    "print(y_test_predicted[y_test_predicted['count']<0])\n",
    "y_test_predicted[y_test_predicted['count']<0] = 0\n",
    "\n",
    "# y_test_predicted에서 상위 10개의 행 확인하기\n",
    "print(pd.DataFrame(y_test_predicted).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1ec2c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 통해 Y_TEST 값을 예측하기\n",
    "Y_TEST_PREDICTED = pd.DataFrame(model.predict(X_TEST)).rename(columns={0:\"count\"})\n",
    "# Y_TEST_PREDICTED의 'count' 칼럼 값이 음수인 데이터를 추출하여, 0dmfh qkRnrl\n",
    "Y_TEST_PREDICTED[Y_TEST_PREDICTED['count']<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e4be40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지의 metrics 모듈에서 r2_score 함수 가져오기\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "60964e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044130641900863\n"
     ]
    }
   ],
   "source": [
    "# 1차 학습 모델의 R2평가지표 값을 확인하기 \n",
    "print(r2_score(Y_TEST, Y_TEST_PREDICTED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "837811fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=10,\n",
       "             reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두번째 방법\n",
    "model = XGBRegressor(n_estimators=200, max_depth=5, random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "24fa379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 통해 y_test값을 예측하기 : 최종 결과 제출용\n",
    "y_test_predicted = pd.DataFrame(model.predict(x_test)).rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d0846db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count\n",
      "0   21.486200\n",
      "1    0.061608\n",
      "2    0.000000\n",
      "3    2.039008\n",
      "4    2.553952\n",
      "5   10.101068\n",
      "6   33.650478\n",
      "7  109.718521\n",
      "8  248.734039\n",
      "9  145.416061\n"
     ]
    }
   ],
   "source": [
    "# y_test_predicted의 'count' 칼럼 값이 음수인 데이터를 추출하여, 0으로 바꾸기\n",
    "y_test_predicted[y_test_predicted['count'] <0] = 0\n",
    "# y_test_predicted에서 상위 10개의 행 확인하기\n",
    "print(y_test_predicted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "86a3f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 통해 Y_TEST 값을 예측하기 : 평가지표 계산용\n",
    "Y_TEST_PREDICTED = pd.DataFrame(model.predict(X_TEST)).rename(columns={0:'count'})\n",
    "# Y_TEST_PREDICTED의 'count'칼럼이 음수인 데이터를 추출하여, 0으로 바꾸기\n",
    "Y_TEST_PREDICTED[Y_TEST_PREDICTED['count']<0 ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3f909405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지의 metrics 모듈에서 r2_score 함수를 가져오기\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1118630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9381182890431057\n"
     ]
    }
   ],
   "source": [
    "# 2차 학습 모델의 R2 평가지표 값을 확인하기\n",
    "print(r2_score(Y_TEST, Y_TEST_PREDICTED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "03d658f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                datetime       count\n",
      "0    2011-01-20 00:00:00   21.486200\n",
      "1    2011-01-20 01:00:00    0.061608\n",
      "2    2011-01-20 02:00:00    0.000000\n",
      "3    2011-01-20 03:00:00    2.039008\n",
      "4    2011-01-20 04:00:00    2.553952\n",
      "...                  ...         ...\n",
      "6488 2012-12-31 19:00:00  204.254379\n",
      "6489 2012-12-31 20:00:00  142.610077\n",
      "6490 2012-12-31 21:00:00  132.809982\n",
      "6491 2012-12-31 22:00:00   92.404938\n",
      "6492 2012-12-31 23:00:00   38.964684\n",
      "\n",
      "[6493 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# x_test_datetime 변수와 y_test_predicted 변수를 세로 방향으로 붙이기\n",
    "print(pd.concat([x_test_datetime, y_test_predicted], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86598248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 출력결과를 final 변수에 저자하기\n",
    "final = pd.concat([x_test_datetime, y_test_predicted], axis=1)\n",
    "# final 변수를 data 디렉터리 하위에 12345.csv 이름으로 저장하기\n",
    "final.to_csv('data/12345.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "07653be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9381182890431057\n",
      "                 datetime       count\n",
      "0     2011-01-20 00:00:00   21.486200\n",
      "1     2011-01-20 01:00:00    0.061608\n",
      "2     2011-01-20 02:00:00    0.000000\n",
      "3     2011-01-20 03:00:00    2.039008\n",
      "4     2011-01-20 04:00:00    2.553952\n",
      "...                   ...         ...\n",
      "6488  2012-12-31 19:00:00  204.254380\n",
      "6489  2012-12-31 20:00:00  142.610080\n",
      "6490  2012-12-31 21:00:00  132.809980\n",
      "6491  2012-12-31 22:00:00   92.404940\n",
      "6492  2012-12-31 23:00:00   38.964684\n",
      "\n",
      "[6493 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 최종 제출 코드\n",
    "# 데이터 가져오기\n",
    "import pandas as pd\n",
    "x_train = pd.read_csv('bigData-main/bike_x_train.csv', encoding='ms949')\n",
    "x_test = pd.read_csv('bigData-main/bike_x_test.csv', encoding='ms949')\n",
    "y_train = pd.read_csv('bigData-main/bike_y_train.csv')\n",
    "\n",
    "# 전처리하기\n",
    "x_train['datetime'] = pd.to_datetime(x_train['datetime'])\n",
    "x_train['year'] = x_train['datetime'].dt.year\n",
    "x_train['hour'] = x_train['datetime'].dt.hour\n",
    "x_test['datetime'] = pd.to_datetime(x_test['datetime'])\n",
    "x_test['year'] = x_test['datetime'].dt.year\n",
    "x_test['hour'] = x_test['datetime'].dt.hour\n",
    "x_test_datetime = x_test['datetime']\n",
    "x_train = x_train.drop(columns=['datetime'])\n",
    "x_test = x_test.drop(columns=['datetime'])\n",
    "y_train = y_train.drop(columns=['datetime'])\n",
    "\n",
    "# 데이터 분리하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(x_train, y_train, test_size=0.2, random_state=10)\n",
    "\n",
    "# 모델을 학습하고 테스트 데이터의 종속변수 값을 예측하기\n",
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(n_estimators=200, max_depth=5, random_state=10)\n",
    "model.fit(X_TRAIN, Y_TRAIN)\n",
    "y_test_predicted = pd.DataFrame(model.predict(x_test)).rename(columns={0:'count'})\n",
    "y_test_predicted[y_test_predicted['count'] < 0] = 0\n",
    "Y_TEST_PREDICTED = pd.DataFrame(model.predict(X_TEST)).rename(columns={0:'count'})\n",
    "Y_TEST_PREDICTED[Y_TEST_PREDICTED['count']<0] = 0\n",
    "\n",
    "# 모델 평가하기\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(Y_TEST, Y_TEST_PREDICTED))\n",
    "\n",
    "# 결과 제출하기\n",
    "final = pd.concat([x_test_datetime, y_test_predicted], axis=1)\n",
    "final.to_csv('data/12345.csv', index=False)\n",
    "\n",
    "print(pd.read_csv('data/12345.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ade53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
